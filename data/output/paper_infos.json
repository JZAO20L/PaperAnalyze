[
    {
        "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
        "model_type": "黑盒",
        "model_names": [
            "AdvWeb",
            "GPT-4V",
            "Gemini 1.5"
        ],
        "method_description": "AdvWeb 是一种新颖的黑盒攻击框架，旨在通过生成和注入不可见的对抗性提示到网页中，误导 VLM 驱动的网络代理执行特定的有害行为（如错误的金融交易或不适当的股票购买）。该方法通过强化学习优化对抗性提示字符串，并确保攻击的隐蔽性和可控性。",
        "main_result": "在各种任务和网站领域中，AdvWeb 对基于 GPT-4V 和 Gemini 1.5 的 VLM 网络代理实现了显著更高的攻击成功率（平均达到 97.5% 和 99.8%），显著优于现有基线方法。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为AdvWeb的框架，用于对基于VLM的网络代理进行可控的黑盒攻击。比赛的目标是设计对抗性文本和图像以触发基础模型的安全风险输出，而AdvWeb的核心功能正是生成对抗性字符串并将其注入HTML内容中，误导网络代理执行特定的有害操作。此外，AdvWeb强调了其生成的对抗性内容具有隐蔽性和可控性，这与比赛中要求的对抗性文本设计目标一致。因此，该论文的方法非常适合用于本次比赛。"
    },
    {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "model_type": "黑盒",
        "model_names": [
            "LLaVA-1.5",
            "InstructBLIP"
        ],
        "method_description": "通过利用多智能体系统的交互和内存存储特性，实现一种新的‘传染性越狱’攻击方式。该方法通过注入一个对抗性图像到单个智能体的内存库中，进而导致几乎所有的其他智能体在无进一步干预的情况下快速感染并表现出有害行为。",
        "main_result": "实验表明，即使在一个包含多达一百万个智能体的多智能体环境中，只需向单个随机选择的智能体注入一个对抗性图像即可实现传染性越狱，所有受感染的智能体在27到31轮对话后会自动展现出有害行为。此外，研究还提出了一种简单原则来判断防御机制是否能有效遏制这种传播，但如何设计实际可行的防御策略仍是一个开放问题。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为“传染性越狱”的方法，通过向多智能体环境中的单一智能体注入对抗性图像，能够以指数速度感染几乎所有其他智能体，使其表现出有害行为。这与比赛的目标密切相关，即设计对抗性文本和图像来触发基础模型生成安全风险输出。论文中提到的方法可以通过设计一个对抗性图像（不超过100字）和对抗性文本，形成图像-文本对，用于比赛中的有害文本查询任务。因此，该论文的方法适合用于本次比赛。"
    },
    {
        "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
        "model_type": "黑盒",
        "model_names": [
            "GPT-4",
            "Bing Chat",
            "Google Bard",
            "Spark",
            "ERNIE Bot",
            "MiniGPT-4",
            "Qwen-VL",
            "VisualGLM",
            "BLIP",
            "LLaVA"
        ],
        "method_description": "提出了一种名为Arondight的红队框架，用于评估视觉语言模型的安全性。该框架通过生成多模态监狱突破提示（包括图像和文本），以覆盖多种禁止场景。此外，引入了熵奖励和新颖性奖励指标来增强多样性生成，并结合强化学习引导红队生成更广泛的测试用例。",
        "main_result": "在对十个开源和黑盒视觉语言模型的评估中，Arondight在所有十四种禁止场景中达到了平均84.5%的成功率，显著优于现有单模态和多模态攻击方法。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为Arondight的红队框架，专门用于评估大型视觉语言模型（VLMs）的安全性能。论文中的方法设计了自动化生成的多模态越狱攻击策略，涵盖了图像和文本模态，并且能够针对多种有害内容场景生成对抗性输入。这些特点与比赛题目高度契合，比赛要求设计对抗性文本和图像以触发基础模型生成安全风险输出。因此，论文中的方法非常适合用于本次比赛。"
    },
    {
        "title": "Can Large Language Models Automatically Jailbreak GPT-4V?",
        "model_type": "黑盒",
        "model_names": [
            "GPT-4V",
            "GPT-4",
            "GPT-3.5"
        ],
        "method_description": "提出了一种名为AutoJailbreak的方法，利用大型语言模型（LLMs）进行红队测试以优化越狱提示，并采用弱到强上下文学习提示来提高效率。此外，还引入了一种有效的搜索方法，结合早期停止策略以最小化优化时间和令牌消耗。",
        "main_result": "AutoJailbreak显著优于传统方法，在黑盒条件下实现了超过95.3%的攻击成功率（ASR）。",
        "evaluation": "是",
        "evaluation_reason": "该论文介绍了一种名为AutoJailbreak的技术，旨在通过生成对抗性文本和图像来突破目标模型的安全限制。这与比赛的要求高度相关，因为比赛需要设计对抗性文本（不超过100字）和对抗性图像以触发基础模型生成安全风险输出。此外，论文中的方法已经证明可以显著提高攻击成功率（ASR超过95.3%），并且适用于多模态大型语言模型（MLLMs）。因此，该论文的方法非常适合用于本次比赛。"
    },
    {
        "title": "FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts",
        "model_type": "黑盒",
        "model_names": [
            "LLaVA-v1.5",
            "MiniGPT4",
            "CogVLM-Chat-v1.1",
            "GPT-4V",
            "GPT-4o"
        ],
        "method_description": "FigStep 是一种通过将有害文本指令转换为图形化视觉提示来绕过底层语言模型安全对齐的方法。该方法无需梯度信息，仅依赖于黑盒访问。",
        "main_result": "在六个开源大型视觉语言模型上实现了平均82.50%的攻击成功率，并揭示了当前模型在跨模态安全对齐上的漏洞。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为FigStep的方法，通过将有害文本指令转换为图像提示来绕过大视觉语言模型（LVLMs）的安全对齐。比赛题目要求设计对抗性文本和图像以触发基础模型生成安全风险输出，而FigStep正是利用图像-文本对实现类似目标。论文中描述的方法可以直接应用于比赛中提到的风险类别下的有害文本查询，并生成相应的对抗性图像和文本。因此，该论文的方法适合用于本次比赛。"
    },
    {
        "title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
        "model_type": "黑盒",
        "model_names": [
            "LLMs",
            "MLLMs"
        ],
        "method_description": "本文综述了针对LLMs和MLLMs的越狱攻击研究，总结了评估基准、攻击技术和防御策略的最新进展，并探讨了多模态领域的限制和潜在研究方向。",
        "main_result": "相比单模态越狱攻击，多模态领域仍需进一步探索，本文为未来的多模态越狱防御研究提供了启发，并旨在增强MLLMs的鲁棒性和安全性。",
        "evaluation": "是",
        "evaluation_reason": "该论文全面概述了针对LLMs和MLLMs的越狱攻击和防御策略的研究，包括对抗性文本和图像生成的方法。这些内容与比赛题目密切相关，因为比赛要求设计对抗性文本和图像以触发基础模型的安全风险输出。论文中提到的非参数化攻击（如行为限制、上下文虚拟化、注意力分散等）以及多模态攻击方法（如构造竞争目标、诱导不匹配泛化等）可以直接为比赛中的对抗性文本和图像设计提供理论和技术支持。因此，该论文适合用于本次比赛。"
    },
    {
        "title": "Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything",
        "model_type": "黑盒",
        "model_names": [
            "GPT-4o",
            "GPT-4V",
            "MiniCPM-V2",
            "LLAVA-V1.5 (7B)",
            "LLAVA-V1.6 (7B)",
            "MiniCPM-Llama3-V2.5",
            "Qwen-Chat-VL"
        ],
        "method_description": "通过设计包含恶意行为逻辑图的Flow-JD数据集来评估视觉语言模型的逻辑理解能力，并利用这些能力实现图像到文本的越狱攻击。",
        "main_result": "所有评估的视觉语言模型在手动创建的数据集（Flow-HJD）上的越狱率显著高于AI生成的数据集（Flow-SJD），其中GPT-4o和GPT-4V分别达到了92.8%和70%的越狱成功率，揭示了当前视觉语言模型在逻辑图越狱方面的显著漏洞。",
        "evaluation": "是",
        "evaluation_reason": "该论文研究了视觉语言模型（VLMs）在逻辑流程图上的越狱能力，并设计了一个专门的评估数据集（Flow-JD）。这与比赛的目标高度相关，因为比赛要求设计对抗性文本和图像以触发基础模型生成安全风险输出。论文中的方法可以为比赛提供以下支持：1) 提供了一个逻辑流程图越狱的数据集，可用于生成对抗性图像；2) 分析了不同模型在面对有害行为描述时的脆弱性，这有助于理解如何设计有效的对抗性输入；3) 强调了提示设计对越狱成功率的影响，这对比赛中设计对抗性文本至关重要。因此，该论文的方法适合用于本次比赛。"
    },
    {
        "title": "JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL ATTACKS ON MULTI-MODAL LANGUAGE MODELS",
        "model_type": "Black-box",
        "model_names": [
            "CLIP",
            "LLaV A",
            "LLaMA-Adapter V2"
        ],
        "method_description": "提出了一种基于嵌入空间的组合式对抗攻击方法，通过将恶意触发器隐藏在看似无害的图像中，并结合通用文本提示来突破视觉语言模型的安全机制。该方法无需访问语言模型本身，仅依赖于开放源代码的视觉编码器（如CLIP）。攻击利用四种不同的嵌入空间目标策略，包括纯文本触发器、OCR文本触发器、视觉触发器和组合的OCR文本-视觉触发器。",
        "main_result": "实验表明，基于图像的触发器（尤其是OCR文本与视觉触发器的组合）在突破多模态模型安全方面表现出了显著的成功率，而纯文本触发器的效果较差。攻击展示了跨模态对齐漏洞的风险，并强调了为多模态模型开发新对齐方法的必要性。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种针对多模态语言模型的组合对抗攻击方法，能够生成对抗性图像和文本以触发模型生成有害内容。这与比赛的要求高度相关，因为比赛需要设计对抗性文本和图像对来引发基础模型的安全风险输出。论文中的方法可以通过优化图像和文本的组合来实现这一目标，因此非常适合用于本次比赛。"
    },
    {
        "title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
        "model_type": "白盒",
        "model_names": [
            "MiniGPT-4",
            "LLaV A",
            "InstructBLIP",
            "Gemini",
            "ChatGLM",
            "Qwen",
            "ERNIE Bot"
        ],
        "method_description": "提出了一种名为Bi-Modal Adversarial Prompt Attack (BAP) 的新框架，通过同时优化视觉和文本提示来突破大型视觉语言模型（LVLM）的安全限制。",
        "main_result": "实验表明，BAP在多个数据集和LVLM上显著优于其他方法，平均攻击成功率提高了29.03%。此外，该方法还能成功攻击商用LVLM，如Gemini、Qwen、ChatGLM和ERNIE Bot，并可用于评估模型的偏见和鲁棒性。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为BAP（Bi-Modal Adversarial Prompt Attack）的方法，旨在通过同时优化文本和视觉提示来绕过对齐模型的安全防护。这种方法与比赛题目高度相关，因为比赛要求设计对抗性文本和图像以触发基础模型生成安全风险输出。论文中详细描述了如何生成对抗性文本和图像，并且在多种场景下进行了实验验证，包括针对不同类型的有害内容进行攻击的成功率评估。因此，论文中的方法非常适合用于本次比赛。"
    },
    {
        "title": "Jailbreaking Attack against Multimodal Large Language Model",
        "model_type": "白盒",
        "model_names": [
            "MiniGPT-4",
            "MiniGPT-v2",
            "LLaVA",
            "InstructBLIP",
            "mPLUG-Owl2"
        ],
        "method_description": "提出了一种基于最大似然的攻击方法，通过修改对抗攻击的目标函数，生成图像型越狱提示(imgJP)，使多模态大语言模型(MLLMs)生成有害响应。此外，还引入了一种基于构建的方法，将MLLM越狱技术应用于文本型越狱提示(txtJP)生成，以提高文本大语言模型(LLMs)的越狱效率。",
        "main_result": "该方法展示了强大的数据通用性，能够跨多个未见过的提示和图像进行越狱；同时具备显著的模型迁移能力，在黑盒场景下对多种模型有效。此外，该方法在LLMs越狱中表现出了比现有最先进方法更高的效率。",
        "evaluation": "是",
        "evaluation_reason": "该论文研究了针对多模态大语言模型的越狱攻击方法，包括生成对抗性文本和图像，这与比赛要求设计对抗性文本和图像以触发基础模型生成安全风险输出的目标高度相关。论文提出的方法能够生成imgJP（图像越狱提示）和txtJP（文本越狱提示），这些技术可以直接应用于比赛任务中，帮助参赛者设计有效的对抗性输入。因此，这篇论文非常适合用于本次比赛。"
    },
    {
        "title": "Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character",
        "model_type": "黑盒",
        "model_names": [
            "LLaVA-V1.6-Mistral-7B",
            "Qwen-VL-Chat (7B)",
            "OmniLMM (12B)",
            "InternVL-Chat-V1.5",
            "Gemini-1.0-Pro-Vision"
        ],
        "method_description": "提出了一种名为Visual Role-play (VRP)的新方法，通过角色扮演生成高风险人物图像，并结合角色描述和恶意问题，诱导多模态大型语言模型生成有害响应。",
        "main_result": "VRP在多种模型上实现了最高的攻击成功率（ASR），平均比最强的基线方法Query relevant和FigStep高出14.3%，并且在通用设置下展示了强大的泛化能力。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种名为Visual Role-play (VRP)的方法，旨在通过角色扮演图像误导多模态大语言模型生成有害内容。这种方法与比赛题目高度相关，因为比赛要求设计对抗性文本和图像以触发模型生成安全风险输出。VRP方法正是通过生成高风险角色图像和对应的文本指令来实现这一目标，并且在多个基准测试中表现出色。此外，论文中的实验设置和结果评估也符合比赛的要求，因此该论文的方法适合用于本次比赛。"
    },
    {
        "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
        "model_type": "白盒",
        "model_names": [
            "MiniGPT-4"
        ],
        "method_description": "提出了一种文本-图像多模态攻击策略，通过双优化目标（注入毒性语义到对抗性图像前缀和生成肯定响应）来破解大型视觉语言模型。",
        "main_result": "该方法在MiniGPT-4上实现了96%的成功率，显著优于现有的单模态攻击方法。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出了一种针对大型视觉-语言模型的文本-图像多模态攻击策略，旨在通过注入对抗性图像前缀和对抗性文本后缀来生成有害内容。这种方法与比赛的目标高度相关，因为比赛要求设计对抗性文本和图像以触发基础模型生成安全风险输出。论文中的方法可以被直接应用于比赛任务，例如生成对抗性文本和图像对以绕过模型的安全机制。因此，这篇论文的方法非常适合本次比赛。"
    },
    {
        "title": "Zer0-Jack: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models",
        "model_type": "黑盒",
        "model_names": [
            "MiniGPT-4",
            "LLaVA1.5",
            "INF-MLLM1",
            "GPT-4o"
        ],
        "method_description": "提出了一种利用零阶优化技术生成恶意图像的方法，用于直接攻击黑盒多模态大型语言模型（MLLMs）。该方法通过仅优化图像的部分区域来减少内存使用，并显著提高了攻击成功率。",
        "main_result": "在各种模型上实现了高攻击成功率，超过了基于转移的攻击方法，并且与现有的白盒攻击方法表现相当。例如，在MiniGPT-4上的攻击成功率为95%，并且可以直接攻击商业MLLMs如GPT-4o。",
        "evaluation": "是",
        "evaluation_reason": "该论文提出的方法 Zer0-Jack 可以直接生成对抗性图像和文本，用于攻击黑盒多模态大语言模型（MLLMs），并且在多种模型上展现了高成功率。比赛要求设计对抗性文本和图像以触发指定基础模型生成安全风险输出，而 Zer0-Jack 的功能与此高度契合。此外，Zer0-Jack 不需要白盒访问，降低了实现难度，并且能够直接应用于商业 MLLMs，例如 GPT-4o，这使得它非常适合本次比赛的需求。"
    }
]